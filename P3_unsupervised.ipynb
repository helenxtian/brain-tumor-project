{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unsupervised Learning**\n",
    "\n",
    "**Data preparation**\n",
    "- convert images into feature vectors (ensure normalization for normalization and preprocessing for consistency)\n",
    "\n",
    "**Clustering**\n",
    "- explore K-means, hierarchical clustering, or DBSCAN to see which one is best at identify patterns\n",
    "- choose optimal number of clusters (K) using techniques like the elbow method or silhouette score\n",
    "- evaluate performance and visualize clustering results for insights into data distribution and cluster relationships\n",
    "\n",
    "**PCA**\n",
    "- reduce dimensionality and select number of principal components based on variance ratio\n",
    "- analyze contribution of each component and assess impact on data structure\n",
    "\n",
    "**(Optional) combination of clustering and pca**\n",
    "- compare performance using original features vs. PCA-reduced features\n",
    "\n",
    "**Integration with supervised learning**\n",
    "- explore clustering and pca results (and the optional combination of them) as additional inputs to P2_supervised model (based off EfficientNet)\n",
    "- evaluate enhancement of supervised learning performance\n",
    "\n",
    "**Analyze Model Performance**\n",
    "- plot loss and accuracy over epochs to visualize training progress and identify potential overfitting or underfitting.\n",
    "- create a confusion matrix to examine how well the model distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from datasets import BrainTumorMRIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    NUM_CLASSES = 4\n",
    "    EPOCHS = 16\n",
    "    BATCH_SIZE = (\n",
    "        32 if torch.cuda.device_count() < 2 \n",
    "        else (32 * torch.cuda.device_count())\n",
    "    )\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    DATASET_PATH = './data/original/'\n",
    "    TRAIN_PATH = './data/original//Training/'\n",
    "    TEST_PATH = './data/original/Testing/'\n",
    "    \n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "test_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('/')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    df = pd.DataFrame({'image_path': image_paths, 'label': generate_labels(labels)})\n",
    "    return df\n",
    "\n",
    "def _load(image_path, as_tensor=True, target_size=(224,224)):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize image to target size and convert to RGB\n",
    "    image = image.resize(target_size)\n",
    "    image = image.convert('RGB')\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([transforms.ToTensor(), transforms.Grayscale()])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = build_df(train_images, generate_labels(train_images))\n",
    "test_df = build_df(test_images, generate_labels(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sample03 = train_df.sample(frac=0.3, random_state=42).reset_index(drop=True)\n",
    "train_split_idx, val_split_idx, _, _ = (train_test_split(\n",
    "    train_df_sample03.index,\n",
    "    train_df_sample03.label,\n",
    "    test_size=0.35, \n",
    "    stratify=train_df_sample03.label,\n",
    "    random_state=CFG.SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1114, 2), (600, 2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new_df = train_df_sample03.iloc[train_split_idx].reset_index(drop=True)\n",
    "val_df = train_df_sample03.iloc[val_split_idx].reset_index(drop=True)\n",
    "train_new_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples count:\t\t1114\t(36.83%)\n",
      "validation samples count:\t600\t(19.83%)\n",
      "test samples count:\t\t1311\t(43.34%)\n",
      "================================================\n",
      "TOTAL:\t\t\t\t3025\t(100.00%)\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_new_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "total = train_size + val_size + test_size\n",
    "\n",
    "# View the counts\n",
    "print(f'train samples count:\\t\\t{train_size}\\t({(100 * train_size/total):.2f}%)')\n",
    "print(f'validation samples count:\\t{val_size}\\t({(100 * val_size/total):.2f}%)')\n",
    "print(f'test samples count:\\t\\t{test_size}\\t({(100 * test_size/total):.2f}%)')\n",
    "print('================================================')\n",
    "print(f'TOTAL:\\t\\t\\t\\t{total}\\t({(100 * total/total):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transoformation definitions\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "idx = random.sample(train_df_sample03.index.to_list(), 1)[0]\n",
    "aug_image = _load(train_df_sample03.image_path[idx], as_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BrainTumorMRIDataset(train_new_df, transform=train_transforms)\n",
    "val_ds = BrainTumorMRIDataset(val_df, transform=test_transforms)\n",
    "test_ds = BrainTumorMRIDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=CFG.APPLY_SHUFFLE\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the EfficientNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "    model_weights = (\n",
    "        torchvision\n",
    "        .models\n",
    "        .EfficientNet_V2_L_Weights\n",
    "        .DEFAULT\n",
    "    )\n",
    "    model = (torchvision.models.efficientnet_v2_l(weights=model_weights)).to(device) \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = get_efficientnetv2_model(CFG.DEVICE)\n",
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE\n",
    "}\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features\n",
    "This following method and definition is the extraction of features from the training data using the EfficientNet model. We will use train_features in our exploration of clustering and pca (disjoint and joint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(CFG.DEVICE)\n",
    "            outputs = model(images)\n",
    "            features.extend(outputs.cpu().numpy())\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_features(train_loader, backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(features, method='kmeans'):\n",
    "    if method == 'kmeans':\n",
    "        kmeans = KMeans(n_clusters=CFG.NUM_CLASSES, random_state=42)\n",
    "        clusters = kmeans.fit_predict(features)\n",
    "    elif method == 'hierarchical':\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=CFG.NUM_CLASSES)\n",
    "        clusters = hierarchical.fit_predict(features)\n",
    "    elif method == 'dbscan':\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        clusters = dbscan.fit_predict(features)\n",
    "    else:\n",
    "        raise ValueError(\"invalid clustering method. choose from 'kmeans', 'hierarchical', or 'dbscan'\")\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on training features\n",
    "train_clusters_kmeans = perform_clustering(train_features, method='kmeans')\n",
    "train_clusters_hierarchical = perform_clustering(train_features, method='hierarchical')\n",
    "train_clusters_dbscan = perform_clustering(train_features, method='dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate clustering labels into the training dataset\n",
    "train_new_df['cluster_kmeans'] = train_clusters_kmeans\n",
    "train_new_df['cluster_hierarchical'] = train_clusters_hierarchical\n",
    "train_new_df['cluster_dbscan'] = train_clusters_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'label', 'cluster_kmeans', 'cluster_hierarchical',\n",
      "       'cluster_dbscan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'clustering'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define new data loaders with clustering labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_ds_cluster_kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mBrainTumorMRIDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_new_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster_kmeans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_ds_cluster_hierarchical \u001b[38;5;241m=\u001b[39m BrainTumorMRIDataset(train_new_df, transform\u001b[38;5;241m=\u001b[39mtrain_transforms, clustering\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_hierarchical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_ds_cluster_dbscan \u001b[38;5;241m=\u001b[39m BrainTumorMRIDataset(train_new_df, transform\u001b[38;5;241m=\u001b[39mtrain_transforms, clustering\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_dbscan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'clustering'"
     ]
    }
   ],
   "source": [
    "# Define new data loaders with clustering labels\n",
    "train_ds_cluster_kmeans = BrainTumorMRIDataset(train_new_df, transform=train_transforms, clustering='cluster_kmeans')\n",
    "train_ds_cluster_hierarchical = BrainTumorMRIDataset(train_new_df, transform=train_transforms, clustering='cluster_hierarchical')\n",
    "train_ds_cluster_dbscan = BrainTumorMRIDataset(train_new_df, transform=train_transforms, clustering='cluster_dbscan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(efficientnet_model.parameters(), lr=CFG.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_epoch(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            eval_loss += loss.item()\n",
    "            predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            eval_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    eval_acc = eval_acc / len(dataloader)\n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    eval_dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    epochs:int,\n",
    "    device:torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    session = {\n",
    "        'loss'          : [],\n",
    "        'accuracy'      : [],\n",
    "        'eval_loss'     : [],\n",
    "        'eval_accuaracy': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        train_loss, train_acc = execute_epoch(model, train_dataloader, optimizer, loss_fn, device)\n",
    "        eval_loss, eval_acc = evaluate(model, eval_dataloader, loss_fn, device)\n",
    "        print(f'loss: {train_loss:.4f} - acc: {train_acc:.4f} - eval_loss: {eval_loss:.4f} - eval_acc: {eval_acc:.4f}')\n",
    "        session['loss'].append(train_loss)\n",
    "        session['accuracy'].append(train_acc)\n",
    "        session['eval_loss'].append(eval_loss)\n",
    "        session['eval_accuaracy'].append(eval_acc)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training EfficientNet Model')\n",
    "print(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\n",
    "print('----------------------------------')\n",
    "\n",
    "efficientnet_session_config = {\n",
    "    'model'               : efficientnet_model,\n",
    "    'train_dataloader'    : train_loader,\n",
    "    'eval_dataloader'     : val_loader,\n",
    "    'optimizer'           : optimizer,\n",
    "    'loss_fn'             : loss_fn,\n",
    "    'epochs'              : CFG.EPOCHS,\n",
    "    'device'              : CFG.DEVICE\n",
    "}\n",
    "\n",
    "efficientnet_session_history = train(**efficientnet_session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model:nn.Module, \n",
    "    sample_loader:torch.utils.data.DataLoader,\n",
    "    device:torch.device) -> np.ndarray:\n",
    "    model.eval() \n",
    "    predictions = []\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(tqdm(sample_loader)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X) \n",
    "            predicted_probs = torch.softmax(y_pred, dim=1).cpu().numpy()\n",
    "            predictions.append(predicted_probs) \n",
    "    return np.vstack(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
