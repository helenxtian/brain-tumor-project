{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083833b-9a4c-4486-a619-33f9d775490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "import glob, warnings, random, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import VGG19_Weights, EfficientNet_V2_L_Weights\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from datasets_supervised import BrainTumorMRIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1ba66-f0d0-4085-a460-3f47a864fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeee119-7623-4662-bb5f-97dbf9ef3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    NUM_CLASSES = 4\n",
    "    EPOCHS = 16\n",
    "    BATCH_SIZE = (\n",
    "        32 if torch.cuda.device_count() < 2 \n",
    "        else (32 * torch.cuda.device_count())\n",
    "    )\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    DATASET_PATH = './data/original/'\n",
    "    TRAIN_PATH = './data/original//Training/'\n",
    "    TEST_PATH = './data/original/Testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951ee48-9889-4aad-8254-a6a9393c9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "test_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91878c-d788-4bab-ba31-0efd4c889d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('/')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    df = pd.DataFrame({'image_path': image_paths, 'label': generate_labels(labels)})\n",
    "    return df\n",
    "\n",
    "def _load(image_path, as_tensor=True, target_size=(224,224)):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize image to target size and convert to RGB\n",
    "    image = image.resize(target_size)\n",
    "    image = image.convert('RGB')\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([transforms.ToTensor(), transforms.Grayscale()])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ca01e-b367-4ca5-8b1c-849aafae92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = build_df(train_images, generate_labels(train_images))\n",
    "test_df = build_df(test_images, generate_labels(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc952933-3b77-4da4-b13b-ec62cf961a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sample = train_df.sample(frac=0.30, random_state=42).reset_index(drop=True)\n",
    "train_split_idx, val_split_idx, _, _ = (train_test_split(\n",
    "    train_df_sample.index,\n",
    "    train_df_sample.label,\n",
    "    test_size=0.20, \n",
    "    stratify=train_df_sample.label,\n",
    "    random_state=CFG.SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce675ee1-ce57-4098-9430-1bb1be114c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_df = train_df_sample.iloc[train_split_idx].reset_index(drop=True)\n",
    "val_df = train_df_sample.iloc[val_split_idx].reset_index(drop=True)\n",
    "train_new_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb3af3-a293-4a54-86eb-2115b87367df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762397ce-c734-498c-9ff6-9f1705adb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_new_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "total = train_size + val_size\n",
    "\n",
    "# View the counts\n",
    "print(f'train samples count:\\t\\t{train_size}\\t({(100 * train_size/total):.2f}%)')\n",
    "print(f'validation samples count:\\t{val_size}\\t({(100 * val_size/total):.2f}%)')\n",
    "print('================================================')\n",
    "print(f'TOTAL:\\t\\t\\t\\t{total}\\t({(100 * total/total):.2f}%)')\n",
    "\n",
    "print('')\n",
    "print(f'INDEPENDENT test samples count:\\t\\t{test_size}\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825232ad-7f74-4efa-ad76-69022440982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "idx = random.sample(train_df_sample.index.to_list(), 1)[0]\n",
    "aug_image = _load(train_df_sample.image_path[idx], as_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cb852-ff1b-4321-a77d-d1ed59f933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_ds = BrainTumorMRIDataset(train_new_df, transform=train_transforms)\n",
    "val_ds = BrainTumorMRIDataset(val_df, transform=test_transforms)\n",
    "test_ds = BrainTumorMRIDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9d2d0-039a-4dc9-8e70-fa21c66b3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=CFG.APPLY_SHUFFLE,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41610479-2a74-4158-b163-34d2e85f5ac6",
   "metadata": {},
   "source": [
    "## Defining and Training the EfficientNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed3032-cd93-4940-bfac-de90cad258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fb912-75be-4d6c-9647-62ed03219973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "    model_weights = (\n",
    "        torchvision\n",
    "        .models\n",
    "        .EfficientNet_V2_L_Weights\n",
    "        .DEFAULT)\n",
    "    model = (torchvision.models.efficientnet_v2_l(weights=model_weights)).to(device) \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2b70d-9d1c-4c4e-a0e2-10a9ce34835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = get_effiecientnetv2_model(CFG.DEVICE)\n",
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE}\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2728727-98e4-467b-9a17-f26b1125e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(efficientnet_model.parameters(), lr=CFG.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fa110-f1d4-4c86-a370-7d1e7cf9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_epoch(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        \n",
    "        # Gradients & Backpropagate Loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute Batch Metrics\n",
    "        predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "        \n",
    "    # Compute Step Metrics\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90a8c0-08a3-434a-bafd-adad52a94c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            eval_loss += loss.item()\n",
    "            predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            eval_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    eval_acc = eval_acc / len(dataloader)\n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed8161-65cd-45b6-b63f-166481adbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    eval_dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    epochs:int,\n",
    "    device:torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    session = {\n",
    "        'loss'          : [],\n",
    "        'accuracy'      : [],\n",
    "        'eval_loss'     : [],\n",
    "        'eval_accuaracy': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        train_loss, train_acc = execute_epoch(model, train_dataloader, optimizer, loss_fn, device)\n",
    "        eval_loss, eval_acc = evaluate(model, eval_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f'loss: {train_loss:.4f} - acc: {train_acc:.4f} - eval_loss: {eval_loss:.4f} - eval_acc: {eval_acc:.4f}')\n",
    "        session['loss'].append(train_loss)\n",
    "        session['accuracy'].append(train_acc)\n",
    "        session['eval_loss'].append(eval_loss)\n",
    "        session['eval_accuaracy'].append(eval_acc)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82640-e62b-424d-af9e-b4a7efda60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training EfficientNet Model')\n",
    "print(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\n",
    "print('----------------------------------')\n",
    "efficientnet_session_config = {\n",
    "    'model'               : efficientnet_model,\n",
    "    'train_dataloader'    : train_loader,\n",
    "    'eval_dataloader'     : val_loader,\n",
    "    'optimizer'           : optimizer,\n",
    "    'loss_fn'             : loss_fn,\n",
    "    'epochs'              : CFG.EPOCHS,\n",
    "    'device'              : CFG.DEVICE\n",
    "}\n",
    "efficientnet_session_history = train(**efficientnet_session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c6bdb-1ce6-4d70-abff-1d8c2b6603a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
