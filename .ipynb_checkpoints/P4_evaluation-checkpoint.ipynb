{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9083833b-9a4c-4486-a619-33f9d775490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "import glob, warnings, random, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.models import VGG19_Weights, EfficientNet_V2_L_Weights\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "from datasets_supervised import BrainTumorMRIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1ba66-f0d0-4085-a460-3f47a864fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eeee119-7623-4662-bb5f-97dbf9ef3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    NUM_CLASSES = 4\n",
    "    EPOCHS = 16\n",
    "    BATCH_SIZE = (\n",
    "        32 if torch.cuda.device_count() < 2 \n",
    "        else (32 * torch.cuda.device_count())\n",
    "    )\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    DATASET_PATH = './data/original/'\n",
    "    TRAIN_PATH = './data/original//Training/'\n",
    "    TEST_PATH = './data/original/Testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4951ee48-9889-4aad-8254-a6a9393c9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "test_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa91878c-d788-4bab-ba31-0efd4c889d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('/')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    df = pd.DataFrame({'image_path': image_paths, 'label': generate_labels(labels)})\n",
    "    return df\n",
    "\n",
    "def _load(image_path, as_tensor=True, target_size=(224,224)):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize(target_size)\n",
    "    image = image.convert('RGB')\n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([transforms.ToTensor(), transforms.Grayscale()])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2ca01e-b367-4ca5-8b1c-849aafae92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = build_df(train_images, generate_labels(train_images))\n",
    "test_df = build_df(test_images, generate_labels(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc952933-3b77-4da4-b13b-ec62cf961a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sample = train_df.sample(frac=0.10, random_state=42).reset_index(drop=True)\n",
    "train_split_idx, val_split_idx, _, _ = (train_test_split(\n",
    "    train_df_sample.index,\n",
    "    train_df_sample.label,\n",
    "    test_size=0.20, \n",
    "    stratify=train_df_sample.label,\n",
    "    random_state=CFG.SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce675ee1-ce57-4098-9430-1bb1be114c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((456, 2), (115, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new_df = train_df_sample.iloc[train_split_idx].reset_index(drop=True)\n",
    "val_df = train_df_sample.iloc[val_split_idx].reset_index(drop=True)\n",
    "train_new_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddb3af3-a293-4a54-86eb-2115b87367df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/original//Training/pituitary/Tr-pi_1405...</td>\n",
       "      <td>pituitary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/original//Training/glioma/Tr-gl_0772.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/original//Training/glioma/Tr-gl_1055.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/original//Training/pituitary/Tr-pi_1305...</td>\n",
       "      <td>pituitary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/original//Training/notumor/Tr-no_0347.jpg</td>\n",
       "      <td>notumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>./data/original//Training/notumor/Tr-no_1319.jpg</td>\n",
       "      <td>notumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>./data/original//Training/pituitary/Tr-pi_0883...</td>\n",
       "      <td>pituitary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>./data/original//Training/glioma/Tr-gl_1286.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>./data/original//Training/glioma/Tr-gl_0093.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>./data/original//Training/meningioma/Tr-me_035...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path       label\n",
       "0    ./data/original//Training/pituitary/Tr-pi_1405...   pituitary\n",
       "1      ./data/original//Training/glioma/Tr-gl_0772.jpg      glioma\n",
       "2      ./data/original//Training/glioma/Tr-gl_1055.jpg      glioma\n",
       "3    ./data/original//Training/pituitary/Tr-pi_1305...   pituitary\n",
       "4     ./data/original//Training/notumor/Tr-no_0347.jpg     notumor\n",
       "..                                                 ...         ...\n",
       "451   ./data/original//Training/notumor/Tr-no_1319.jpg     notumor\n",
       "452  ./data/original//Training/pituitary/Tr-pi_0883...   pituitary\n",
       "453    ./data/original//Training/glioma/Tr-gl_1286.jpg      glioma\n",
       "454    ./data/original//Training/glioma/Tr-gl_0093.jpg      glioma\n",
       "455  ./data/original//Training/meningioma/Tr-me_035...  meningioma\n",
       "\n",
       "[456 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762397ce-c734-498c-9ff6-9f1705adb931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples count:\t\t456\t(79.86%)\n",
      "validation samples count:\t115\t(20.14%)\n",
      "================================================\n",
      "TOTAL:\t\t\t\t571\t(100.00%)\n",
      "\n",
      "INDEPENDENT test samples count:\t\t1311\t\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_new_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "total = train_size + val_size\n",
    "\n",
    "# View the counts\n",
    "print(f'train samples count:\\t\\t{train_size}\\t({(100 * train_size/total):.2f}%)')\n",
    "print(f'validation samples count:\\t{val_size}\\t({(100 * val_size/total):.2f}%)')\n",
    "print('================================================')\n",
    "print(f'TOTAL:\\t\\t\\t\\t{total}\\t({(100 * total/total):.2f}%)')\n",
    "\n",
    "print('')\n",
    "print(f'INDEPENDENT test samples count:\\t\\t{test_size}\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825232ad-7f74-4efa-ad76-69022440982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "idx = random.sample(train_df_sample.index.to_list(), 1)[0]\n",
    "aug_image = _load(train_df_sample.image_path[idx], as_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845cb852-ff1b-4321-a77d-d1ed59f933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_ds = BrainTumorMRIDataset(train_new_df, transform=train_transforms)\n",
    "val_ds = BrainTumorMRIDataset(val_df, transform=test_transforms)\n",
    "test_ds = BrainTumorMRIDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab9d2d0-039a-4dc9-8e70-fa21c66b3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=CFG.APPLY_SHUFFLE,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cad34-73f1-4276-bcea-01b98ea5a06f",
   "metadata": {},
   "source": [
    "## **ORIGINAL MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41610479-2a74-4158-b163-34d2e85f5ac6",
   "metadata": {},
   "source": [
    "### Defining and Training the EfficientNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ed3032-cd93-4940-bfac-de90cad258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param_name, param_value in params.items():\n",
    "            setattr(self, param_name, param_value)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)\n",
    "\n",
    "    def fit(self, train_loader, val_loader, optimizer, loss_fn, epochs):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            # Training\n",
    "            self.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data)\n",
    "                loss = loss_fn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_loss_history.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.eval()\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self(data)\n",
    "                loss = loss_fn(output, target)\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_loss_history.append(val_loss)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "        return train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0fb912-75be-4d6c-9647-62ed03219973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "    model_weights = (\n",
    "        torchvision\n",
    "        .models\n",
    "        .EfficientNet_V2_L_Weights\n",
    "        .DEFAULT)\n",
    "    model = (torchvision.models.efficientnet_v2_l(weights=model_weights)).to(device) \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0c2b70d-9d1c-4c4e-a0e2-10a9ce34835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = get_efficientnetv2_model(CFG.DEVICE)\n",
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE}\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2728727-98e4-467b-9a17-f26b1125e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(efficientnet_model.parameters(), lr=CFG.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57fa110-f1d4-4c86-a370-7d1e7cf9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_epoch(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        \n",
    "        # Gradients & Backpropagate Loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute Batch Metrics\n",
    "        predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "        \n",
    "    # Compute Step Metrics\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d90a8c0-08a3-434a-bafd-adad52a94c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            eval_loss += loss.item()\n",
    "            predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            eval_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    eval_acc = eval_acc / len(dataloader)\n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6ed8161-65cd-45b6-b63f-166481adbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    eval_dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    epochs:int,\n",
    "    device:torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    session = {\n",
    "        'loss'          : [],\n",
    "        'accuracy'      : [],\n",
    "        'eval_loss'     : [],\n",
    "        'eval_accuaracy': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        train_loss, train_acc = execute_epoch(\n",
    "            model, \n",
    "            train_dataloader, \n",
    "            optimizer, \n",
    "            loss_fn, \n",
    "            device)\n",
    "        eval_loss, eval_acc = evaluate(\n",
    "            model, \n",
    "            eval_dataloader,\n",
    "            loss_fn, \n",
    "            device)\n",
    "        print(f'loss: {train_loss:.4f} - acc: {train_acc:.4f} - eval_loss: {eval_loss:.4f} - eval_acc: {eval_acc:.4f}')\n",
    "        session['loss'].append(train_loss)\n",
    "        session['accuracy'].append(train_acc)\n",
    "        session['eval_loss'].append(eval_loss)\n",
    "        session['eval_accuaracy'].append(eval_acc)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075f5ac-f859-40ec-9908-aa8847d305d6",
   "metadata": {},
   "source": [
    "### Original Training of Model (with no Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82640-e62b-424d-af9e-b4a7efda60a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EfficientNet Model\n",
      "Train on 456 samples, validate on 115 samples.\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5804444f3554337b4210015b453dc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bb906373894896b63644bc4e84e949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training EfficientNet Model')\n",
    "print(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\n",
    "print('----------------------------------')\n",
    "efficientnet_session_config = {\n",
    "    'model'               : efficientnet_model,\n",
    "    'train_dataloader'    : train_loader,\n",
    "    'eval_dataloader'     : val_loader,\n",
    "    'optimizer'           : optimizer,\n",
    "    'loss_fn'             : loss_fn,\n",
    "    'epochs'              : CFG.EPOCHS,\n",
    "    'device'              : CFG.DEVICE\n",
    "}\n",
    "efficientnet_session_history = train(**efficientnet_session_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df3253-5f5b-49ad-8c53-05ac3d0b7e3e",
   "metadata": {},
   "source": [
    "### Evaluate the Original Model\n",
    "We will be looking at accuracy and evaluation losses, a confusion matrix (matching true and predicted labels), and the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf4299-0056-44f6-9efc-d2d4002e4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model:nn.Module, \n",
    "    sample_loader:torch.utils.data.DataLoader,\n",
    "    device:torch.device) -> np.ndarray:\n",
    "    model.eval() \n",
    "    predictions = []\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(tqdm(sample_loader)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X) \n",
    "            predicted_probs = torch.softmax(y_pred, dim=1).cpu().numpy()\n",
    "            predictions.append(predicted_probs) \n",
    "    return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf8e3a-1d61-4799-a3d9-6bfe04bd9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_test_probs = predict(efficientnet_model, test_loader, CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59b166-5564-48db-9e8f-8bc12e6b758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history):\n",
    "    loss = np.array(history['loss'])\n",
    "    accuracy = np.array(history['accuracy'])\n",
    "    val_loss = np.array(history['eval_loss'])\n",
    "    val_accuracy = np.array(history['eval_accuaracy'])\n",
    "    epochs = range(len(history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    ax1.plot(epochs, loss, label='training_loss', marker='o')\n",
    "    ax1.plot(epochs, val_loss, label='eval_loss', marker='o')\n",
    "    ax1.fill_between(epochs, loss, val_loss, where=(loss > val_loss), color='C0', alpha=0.3, interpolate=True)\n",
    "    ax1.fill_between(epochs, loss, val_loss, where=(loss < val_loss), color='C1', alpha=0.3, interpolate=True)\n",
    "    ax1.set_title('Loss (Lower Means Better)', fontsize=16)\n",
    "    ax1.set_xlabel('Epochs', fontsize=12)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, accuracy, label='training_accuracy', marker='o')\n",
    "    ax2.plot(epochs, val_accuracy, label='eval_accuracy', marker='o')\n",
    "    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy > val_accuracy), color='C0', alpha=0.3, interpolate=True)\n",
    "    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy < val_accuracy), color='C1', alpha=0.3, interpolate=True)\n",
    "    ax2.set_title('Accuracy (Higher Means Better)', fontsize=16)\n",
    "    ax2.set_xlabel('Epochs', fontsize=12)\n",
    "    ax2.legend();\n",
    "    \n",
    "    sns.despine();\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60951a0a-362c-47d7-9f8c-6ff653c55bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_session_history_df = pd.DataFrame(efficientnet_session_history)\n",
    "efficientnet_session_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29771aa0-edab-4dc7-80bb-c6b9c5433b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(efficientnet_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe9906-4ca9-4344-885d-9eac65e25e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes='auto', figsize=(10, 10), text_size=12): \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=figsize)\n",
    "    disp = sns.heatmap(cm, annot=True, cmap='OrRd', annot_kws={\"size\": text_size}, fmt='g',\n",
    "        linewidths=0.5, linecolor='black', clip_on=False, xticklabels=classes, yticklabels=classes)\n",
    "    disp.set_title('Confusion Matrix', fontsize=24)\n",
    "    disp.set_xlabel('Predicted Label', fontsize=20) \n",
    "    disp.set_ylabel('True Label', fontsize=20)\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d0b6f-84cb-4dbf-9ab1-abda418dc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [*map(test_ds.class_to_idx.get, test_ds.labels)]\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15ee54-56c3-41d5-9939-dcedfbeab3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_test_preds = np.argmax(efficientnet_test_probs, axis=1)\n",
    "print(efficientnet_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3ccda-f57b-400b-8beb-551a33372d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, efficientnet_test_preds, figsize=(8,7), classes=test_ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d501ef-43cf-4454-825b-d2a793963568",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_labels, efficientnet_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496207d-867a-43d2-bdda-b1c3ca56105f",
   "metadata": {},
   "source": [
    "## **Hyperparameter tuning with GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b4166-10c7-404b-b359-449aeea520df",
   "metadata": {},
   "source": [
    "GridSearchCV expects numpy arrays or similar objects for X and y, not PyTorch DataLoaders. We'll transform the data into a format that GridSearchCV can understand, but creating a function that converts a DataLoader to numpy array for X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e32079-5991-4d72-9bb6-486f08a6695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.y is not None:\n",
    "            return sample, self.y[idx]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1830244-782a-47b5-b899-e1ecf86cf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_to_numpy(dataloader):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for data, targets in dataloader:\n",
    "        X_list.append(data.numpy())\n",
    "        y_list.append(targets.numpy())\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa4999-9eee-4cb5-9a7d-64818903ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and validation DataLoader to numpy arrays\n",
    "X_train, y_train = dataloader_to_numpy(train_loader)\n",
    "X_val, y_val = dataloader_to_numpy(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88652b32-e427-4720-a9db-bb8a2db6fc36",
   "metadata": {},
   "source": [
    "GridSearchCV expects the model to follow scikit-learn's interface, which means we won't be able to use the above PyTorch model. As such, we will create a wrapper class for the PyTorch model such that it adheres to the scikit-learn estimator interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e9580-08b9-44bb-827d-f0560a5e60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd235028-afb2-4f21-aa0c-3e43ac4e9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, num_classes=CFG.NUM_CLASSES, dropout_rate=0.2, weight_decay=0.01, momentum=0.9, lr=0.001, device=CFG.DEVICE):\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.model = self._initialize_model()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        model = torchvision.models.efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.DEFAULT)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=self.dropout_rate, inplace=True),\n",
    "            nn.Linear(model.classifier[1].in_features, self.num_classes)\n",
    "        ).to(self.device)\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X).float().to(self.device)\n",
    "        y_tensor = torch.tensor(y).long().to(self.device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(CFG.EPOCHS):\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, targets in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "            print(f'Epoch {epoch+1}/{CFG.EPOCHS}, Loss: {running_loss/len(dataloader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.tensor(X).float().to(self.device)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                outputs = self.model(data[0])\n",
    "                _, predicted = outputs.max(1)\n",
    "                preds.append(predicted.cpu().numpy())\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X_tensor = torch.tensor(X).float().to(self.device)\n",
    "        y_tensor = torch.tensor(y).long().to(self.device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in dataloader:\n",
    "                outputs = self.model(data[0])\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        return 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ba2de-9838-485f-aef7-43d5011df249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.0, 0.1, 0.2],\n",
    "    'weight_decay': [0.0, 0.001, 0.01],\n",
    "    'momentum': [0.9, 0.95],\n",
    "    'lr': [1e-4, 1e-3, 1e-2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03400694-57c4-4119-a37c-5cba57643605",
   "metadata": {},
   "source": [
    "Above, I have listed out a the parameters I want to explore and fine-tune. Below, I will discuss what they and the respective values I have picked mean:\n",
    "1. **dropout_rate:** Dropout is a regularization technique used to prevent overfitting in neural networks. It randomly drops a fraction of the neurons during training, forcing the network to learn more robust features. The dropout_rate parameter controls the probability of dropping neurons during training. A value of 0.0 means no dropout, while values closer to 1.0 mean more aggressive dropout.\n",
    "2. **weight_decay:** Weight decay, also known as L2 regularization, is a regularization technique that penalizes large weights in the model. It adds a term to the loss function proportional to the squared magnitude of the weights. The weight_decay parameter controls the strength of this penalty. A value of 0.0 means no weight decay, while higher values impose stronger regularization by penalizing larger weights more.\n",
    "3. **momentum:** Momentum is a parameter used in optimization algorithms like SGD (Stochastic Gradient Descent) with momentum and variants like Adam. It helps accelerate gradients in the right direction and dampens oscillations. The momentum parameter controls the momentum term in the optimization algorithm. A value close to 1.0 means higher momentum, which helps in faster convergence, while values closer to 0.0 mean less momentum.\n",
    "4. **lr (learning rate):** The learning rate is one of the most crucial hyperparameters in training neural networks. It determines the step size taken during optimization to update the weights of the network. The lr parameter controls the learning rate used in the optimization algorithm. Lower values lead to slower but more precise convergence, while higher values can speed up convergence but risk overshooting the minimum of the loss function.\n",
    "\n",
    "I'm trying to find the right balance between model capacity (dropout_rate), regularization (weight_decay), optimization stability (momentum), and convergence speed (learning rate) to achieve a better performance on my dataset without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cf1d8-1b9b-4d76-a707-a52ac7ca70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer outside the loop\n",
    "# backbone_model = torchvision.models.efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850a121-2c51-4cdd-b662-8da32f4de30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnWrapper(num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "tqdm(grid_search.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09f4b1-2b52-4fa1-806d-8603a1ad1f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c22701-6329-49c7-98bf-f63e632bdb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Best Model Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d0fed-5b4a-43e9-bfea-ed4e525664a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57321539-0e97-448e-b1a2-19e5d95dd258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
